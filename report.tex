% Adapted from https://www.overleaf.com/latex/templates/problem-set-template/bdwzvbkxyjfg

\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage{kpfonts}
\usepackage{graphicx} % for figures
\usepackage{grffile}
\usepackage{amsmath}  % for extended math markup
\usepackage{amssymb}
\usepackage{bm}
\usepackage{xfrac}
\usepackage{enumerate}
\usepackage{float}
\usepackage[bookmarks=false]{hyperref} % for URL embedding

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\Course}{CSE515T: Bayesian Methods in Machine Learning}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% these are common math formatting commands that aren't defined by default
\newcommand{\union}{\cup}
\newcommand{\isect}{\cap}
\newcommand{\ceil}[1]{\ensuremath \left\lceil #1 \right\rceil}
\newcommand{\floor}[1]{\ensuremath \left\lfloor #1 \right\rfloor}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\norm}[1]{\ensuremath \mid\mid #1 \mid\mid}

\DeclareMathOperator*{\argmin}{\arg\min}
\DeclareMathOperator*{\argmax}{\arg\max}

\newenvironment{problem}[2][Problem]
{\ifnum\value{page}>1 \newpage\else\fi\noindent{\bfseries #1}  {\bfseries #2.}}
{\mbox{}\\\vspace{-.5em}\noindent\rule{\textwidth}{0.4pt}\hfil\vspace{1em}}

\numberwithin{equation}{section}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\title{Project Report}
\author{Alexis Park, Jonathan Chen, Kevin Xie \\ \Course}
\maketitle

\section*{Data visualization}

The Branin function is defined as follows:

\begin{equation}
  f(\bm{x}) = a(x_2 - bx_1^2 + cx_1 - r)^2 + s(1 - t)\cos(x_1) + s
  \label{eq:branin}
\end{equation}

With domain X=[-5, 10] x [0, 15]  with 1000 values per dimension, figure 1 shows a heatmap of the value of the Branin function.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/Data Visualization/trueBraninHeatmap.png}
  \caption{Heatmap of True Branin Function Values}
  \label{}
\end{figure}

From the plot above, one can see that the function's values fluctuate in a somewhat sinusoidal manner, with a large minimal "trench" spanning diagonally across the center of the domain with steadly increasing regions along the trench's sides. Therefore the function does not appear to be stationary.

By analyzing the equation, we note that the sinusoidal fluctuations can be attributed to both the added cosine term, and the 6th order polynomial term. We thus attempted to pass the data through a transformation that combined an inverse cosine function (arccos) with the cumulative density function of the normal probability distribution (normal cdf). Specfically, we scaled the data from [0, 1] using the normal cdf and passed the modified data through arccos to obtain a set of transformed data. The resulting plot of the transformed data is shown below.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/Data Visualization/Log_Branin_Heatmap.png}
  \caption{}
  \label{}
\end{figure}
Figure 2 shows more stationary [INSERT MORE INTERPRETATION]

Now, we plotted kernel density estimates of LDA and SVM benchmark data.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{figures/lda_kde.eps}
  \caption{}
  \label{}
\end{figure}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{figures/svm_kde.eps}
  \caption{}
  \label{}
\end{figure}
We plot the kernel density estimates for the SVM and LDA benchmarks below. 
It can be seen that the two estimates have relatively similar relative
behavior but on significantly different scales.

Similar to Branin function, we also took log of the LDA and SVM values to make the performance better. Plotted data is shown in figure 5 and 6.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{figures/lda_kde_log.eps}
  \caption{}
  \label{}
\end{figure}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{figures/svm_kde_log.eps}
  \caption{}
  \label{}
\end{figure}

\section*{Model fitting}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/Model Fitting/GP_Posterior_Mean_vs_True_Branin.png}
  \caption{}
  \label{}
\end{figure}
Q: Compare
the predicted values with the true values. Do you see systematic errors?

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/Model Fitting/GP_Posterior_s2_With_Training_Points.png}
  \caption{}
  \label{}
\end{figure}
Q: Do the values make sense? Does the scale make sense? Does the standard
deviation drop to near zero at your data points?

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/Model Fitting/Z_Score_KDE.png}
  \caption{}
  \label{}
\end{figure}
Based on figure 9, the KDE of Z-score distribution follows approximately standard normal distrubtion with more concentrated peak in the middle.

Now, we repeated model fitting using a log transformation to the output of the Branin function.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/Log Fits/GP_Posterior_vs_Log_True.png}
  \caption{}
  \label{}
\end{figure}
Q: Compare
the predicted values with the true values. Do you see systematic errors?

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/Log Fits/GP_Posterior_S2_and_Training_points.png}
  \caption{}
  \label{}
\end{figure}
Q: Do the values make sense? Does the scale make sense? Does the standard
deviation drop to near zero at your data points?

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/Log Fits/Z_Score_KDE.png}
  \caption{}
  \label{}
\end{figure}


LOGS:
Q:  Does the
marginal likelihood improve? Does the model appear better calibrated?

\section*{Bayesian optimization}

The best-fitting models were selected from the previous experiments. 
Specifically, the Branin Model used a log-transformed dataset, a Constant Mean function, and a Squared Exponential covariance function.
Similarly, the LDA Model used a log-transformed dataset, a Constant Mean Function, and a Rational Quadratic covariance Function.
Finally, the SVM Model used a normal dataset with a Constant Mean Function and a product of the Rational Quadratic and Squared Exponential functions as its covariance function.

We then used the Expected Improvement (EI) Acquisition Function, defined from the course notes as:
\begin{equation}
  a_{ei}(\bm{x}) = (f' - \mu(\bm{x}))\Phi(f';\mu(\bm{x}),K(\bm{x},\bm{x})) + K(\bm{x},\bm{x})\mathcal{N}(f';\mu(\bm{x}),K(\bm{x},\bm{x}))
  \label{eq:Expected Improvement}
\end{equation}
Where $\Phi(\bm{x})$ is the Cumulative Probablity Density of the Normal Distribution, and f' is the minimum value of the current observations.
Note that the EI acquisition function combines both exploitation and exploration by using both the posterior mean and known minimum value, and the posterior standard deviation, respectively. 

Using the previously selected 32 points, a GP model was fit using the aforementioned optmized settings and the following heatmaps of the posterior mean and standard deviation of the Branin function were created:

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/Optimization Plots/EI_FMu_Log.png}
  \caption{Predictive Posterior Mean of the log Branin Function, calculated using a previously optimized GP model trained on 32 Sobol Sequence points. 
  Warmer colors indicate higher values. Note the predicted minimum areas in dark blue at the top left and bottom right corners of the plot.}
  \label{fig:ei-fmu-log}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/Optimization Plots/EI_S_Log.png}
  \caption{Predictive Posterior Standard Deviation of the log Branin Function, calculated usinga previously optimized GP model trained on 32 Sobol Sequence points. 
  Warmer colors indicate higher deviations and imply greater uncertainty in the prediction.}
  \label{fig:ei-fs-log}
\end{figure}

The EI value was then calculated using the posteriors and identified the point [7.658, 0] as the optimal point to test next. 
A heatmap of the EI distribution is shown below:

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/Optimization Plots/EI_Log_Optimal_point.png}
  \caption{Expected Improvement acquisition values of the optimized GP model trained on 32 Sobol Sequence points. Warmer colors indicate higher expected improvement. 
  The maximum expected improvement is marked as a black point and denotes the optimal location for the next observation.}
  \label{fig:ei-optimalPoint-log}
\end{figure}

Analyzing \ref{fig:ei-fmu-log}, \ref{fig:ei-fs-log}, and \ref{fig:ei-optimalPoint-log} above, we reason that the proposed optimal testing point is ideal. 
From the posterior mean, it can be seen that the point [7.658, 0] is within a region of predicted minimal values.
Futhermore, from the posterior standard deviation, it can be seen that the point is also within a region of higher uncertainty and therefore reduced predictive confidence.
Thus, it is plausible that the EI acquisition function would seek to test this area and point in order to identify a possible global minimum and improve confidence in an area of uncertainty. 

The following bayesian active learning experiment was then applied independently to the Branin, LDA and SVM functions:
\begin{enumerate}
  \item 5 initial observations were randomly selected, constituting the initial dataset $\mathcal{D}$
  \item For the Branin Function only, a dense grid of 250,000 points was generated within the domain of the function
  \item A GP model using the respective aforementioned optimized settings was fit to $\mathcal{D}$
  \item A new point \emph{x} was found using the EI acquisition function and the GP predictive posterior
  \item The function value \emph{f(x)} was calculated and the point (\emph{x}, \emph{f(x)}) was added to $\mathcal{D}$
  \item Steps 3-5 were repeated 30 times, resulting in a final dataset $\mathcal{D}$ of 35 points
\end{enumerate}

The performance of each of the above experiments was evaluated using the "gap" measure, defined for minimization as:
\begin{equation}
  \text{gap} = \dfrac{f(\text{best found}) - f(\text{best initial})}{f(\text{maximum}) - f(\text{best initial})}
  \label{eq:gap}
\end{equation}

The gaps for the Branin, LDA, and SVM models were calculated to be 1.0000, 0.7932, and 0.9604, respectively. 
This implies that EI successfully found a global minimum of the Branin function, but missed the global minimum of the LDA and SVM functions.

The above bayesian active learning experiment was then modified as such:
\begin{enumerate}
  \item A seed for the random number generator (RNG) was chosen
  \item 5 initial observations were randomly selected, constituting the initial dataset $\mathcal{D}$
  \item For the Branin Function only, a dense grid of 250,000 points was generated within the domain of the function for use in calculating the GP predictive posterior
  \item A GP model using the respective aforementioned optimized settings was fit to $\mathcal{D}$
  \item A new point \emph{x} was found using the EI acquisition function and the GP predictive posterior
  \item The function value \emph{f(x)} was calculated and the point (\emph{x}, \emph{f(x)}) was added to $\mathcal{D}$
  \item Steps 4-6 were repeated 150 times, resulting in a final dataset $\mathcal{D}$ of 155 points
  \item A new GP model using the respective aforementioned optimized setting was fit to the original initial dataset $\mathcal{D}$, now called $\mathcal{D}$'
  \item A new point \emph{x} was found using the Random Search (RS) acquisition function, which randomly selects a new point
  \item The function value \emph{f(x)} was calculated and the point (\emph{x}, \emph{f(x)}) was added to $\mathcal{D}$'
  \item Steps 8-10 were repeated 150 times, resulting in a final dataset $\mathcal{D}$' of 155 points
\end{enumerate}
The above experiment was repeated 20 times with 20 different RNG seeds to create different random initializations for the Branin, LDA, and SVM functions, resulting in a total of 60 experiments. 
The learning curves of the 60 experiments using only the first 30 new observations for both EI and RS acquisition are shown in the figures below:
\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/optimization_log_branin.eps}
  \caption{Learning curves for 20 branin function experiments. The EI acquisition function is shown in blue, and the RS acquisition function is shown in orange. 
  Observe that EI consistently outperforms RS in both rate of GAP increase, and final GAP convergence at 30 observations.}
  \label{fig:ei-v-rand-branin}
\end{figure}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/optimization_log_lda.eps}
  \caption{Learning curves for 20 LDA function experiments. The EI acquisition function is shown in blue, and the RS acquisition function is shown in orange. 
  Note that EI and RS both consistently converge towards the same GAP value at 30 observations}
  \label{fig:ei-v-rand-lda}
\end{figure}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/optimization_norm_svm.eps}
  \caption{Learning curves for 20 SVM function experiments. The EI acquisition function is shown in blue, and the RS acquisition function is shown in orange.
  It can be seen that EI performs at least as well as RS in the majority of experiments, but can find and get stuck in a local minima, as seen in Seed 18.}
  \label{fig:ei-v-rand-svm}
\end{figure}
From figures \ref{fig:ei-v-rand-branin}, and \ref{fig:ei-v-rand-svm} above, it can be seen that EI significantly outperforms RS on both the Branin Function and the SVM Function.
Note that in these experiments, EI often converges to its final GAP value in significantly fewer observations than RS does, and that its final GAP values at 30 observations are generally higher than those of RS.
This behavior was expected, as EI combines both exploitation and exploration together to identify locations with higher expected benefit, whereas RS only applies (random) exploration.
The addition of exploitation and non-random exploration in EI allows it to actually navigate towards optima, whereas RS can only approach optima through luck. 
However, there are cases where EI and RS have similar performance: from figure \ref{fig:ei-v-rand-lda}, it can be seen that the two acqusition functions perform similarly to each other in the majority of experiments at 30 observations. 
We believe that EI's lack of performance on the LDA function can be attributed to a large amount of local minima; EI can often become stuck in local minima, thereby decreasing its performance, whereas RS is purely exploratory and therefore unhindered by the presence of any local optima.

The difference in performance between EI and RS can be further seen when one considers their mean gaps at 30, 60, 90, 120 and 150 observations, 
as shown in the table below:
\begin{center}
  \begin{tabular}{ |c|c|c| } 
   \hline
   Observations & EI (Branin)& RS (Branin) & EI (LDA)& RS (LDA) & EI (SVM)& RS (SVM)\\ 
   \hline
   30 & 0.8072 & 0.3409 & 0.4877 & 0.7122 & 0.8106 & 0.3728 \\ 
   \hline
   60 & 0.8532 & 0.6086 & 0.8611 & 0.7944 & 0.8415 & 0.5676 \\ 
   \hline
   90 & 0.8532 & 0.7124 & 0.8753 & 0.8174 & 0.8592 & 0.8190 \\ 
   \hline
   120 & 0.8532 & 0.8096 & 0.8753 & 0.8633 & 0.9018 & 0.8716 \\ 
   \hline
   150 & 0.8532 & 0.8202 & 0.9473 & 0.9016 & 0.9018 & 0.8737 \\ 
  \end{tabular}
  \caption{Mean Gaps of EI and RS on the Branin, LDA, and SVM Functions at 30, 60, 90, 120 and 150 observations. 
  Note that EI reaches a higher Gap at 150 observations in all functions when compared to RS. Furthermore, for the Branin and SVM functions, 
  EI achieves these higher values significantly faster than RS.}
  \label{}
\end{center}
In all cases, it is evident that EI outperforms RS on all three functions, including LDA, at 150 observations. 
Additionally, EI requires less observations to achieve higher gap values, implying a significantly faster learning rate than RS.
We also note that EI likely spends considerable time navigating local minima in the LDA function, 
as evident by its noticeably worse performance at 30 observations when compared to that of RS. 
However, EI for LDA rapidly improves following this local-minima exploration and quickly outperforms RS. 
It is also important to note that on average, no method was able to find the global minima. 
This result was expected for RS, which would need to be extraordinarily lucky to find the global minima, but was somewhat surprising for EI.
It appears that in the limit, EI may become entrenched in local minima and thus converges towards a suboptimal result. 

We more rigorously compare the performance of EI and RS using a paired t-test. The null hypothesis of the test 


\section*{Bonus}
For the bonus section, we implemented two more acquisition functions: Lower
Confidence Bound (LCB) and Max Variance. We then compared their performances
with EI when used on their own and with two heuristics.

First, we implemented a wrapper for aquisition functions that selects a point
at random with probability $p = 0.1$; we dubbed this heuristic ``random
restarts'' (RR). We also tried each aquisition function while minimizing the
model's hyperparameters after each iteration; we called this ``online
optimization'' (OO). 


\end{document}
